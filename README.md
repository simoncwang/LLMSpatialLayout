# LLM Spatial Layout

Using LLM structured output capabilities to generate reliable spatial layouts from image descriptions. An extension of the GPT4 based box layout generation from the amazing [Grounded Text-to-Image Synthesis with Attention Refocusing](https://attention-refocusing.github.io/) paper.

## Approach & Motivation

In the original Attention Refocusing paper, layouts in the form of bounding boxes were generated by prompting GPT4. This was implemented through using in-context examples to ask the model to generate box coordinates for each object in a description of an image.

As mentioned in the limitations section of the paper, **GPT4 sometimes produces invalid layouts or very small bounding boxes.** This is because at the time the paper was written, structured output capability of models was not yet refined or very robust.
Looking at the code, despite extensive prompting and in-context examples, **the model isn't actually strictly enforced to follow a specific format in its output**, leading to unexpected results at times.

To mitigate this issue, I re-implemented the layout generation scripts using both [OpenAI's structured outputs beta](https://platform.openai.com/docs/guides/structured-outputs?context=ex1#how-to-use) as well as [Ollama structured outputs](https://ollama.com/blog/structured-outputs) to enable use of open-source models.

### Improvements

* Almost always ensures consistent output format to allow for **reliable layout generation**
* **Simplifies prompting and code** in general, reduces the need for extensive in-context examples to enforce output structure
* Use of **open-source models** makes the attention refocusing method more accessible (free and doesn't require API subscription) to allow more users to experiment locally

## Running the code

### Setup

Create a conda environment:
    
    conda create -n "llm-layout" python=3.13

Install the required packages
    
    pip install -r requirements.txt

If using Ollama, first check that your Ollama version is >= 0.5.1 because structured outputs are only available in newer versions
    
    ollama --version

If your version is older, try to upgrade your version as specified here: [Ollama docs](https://github.com/ollama/ollama/blob/main/docs/faq.md). I had to manually uninstall and re-install a new version (0.5.4) from the Ollama website.

Then, be sure to pull the models you want to use first before running the scripts.

    ollama pull [model name]

I used the following for my short experiment:
* llama3:8b
* llama3.1:8b
* qwen2.5:7b

For OpenAI, first set your OpenAI API key:

        export OPENAI_API_KEY="your_api_key_here"

for Windows:

        setx OPENAI_API_KEY "your_api_key_here"

### Generating layouts

#### Commands

Both scripts (OpenAI and Ollama) run the same, simply specify the model as a command line argument. Example here shown for qwen2.5:7b through ollama.

        python3 generate_layout_ollama.py --model qwen2.5:7b
    
When using the OpenAI script **please only use the following models** as specified in the [OpenAI API Docs](https://platform.openai.com/docs/guides/structured-outputs?context=ex1#how-to-use)
* o1-2024-12-17 and later
* gpt-4o-mini-2024-07-18 and later
* gpt-4o-2024-08-06 and later

#### Usage

Once the script is run, you will first be prompted for a description of your desired image. Example: "Three colorful parrots perching on cherry blossom tree branch"

Then, enter a name for the file you want the result to be saved as (you do not need to specify an extension, just the name). Example: "threeparrots"

Finally, the **raw structured output will be printed out in the terminal for reference**, and the image with drawn/labeled bounding boxes will be **saved in the ./outputs folder.**


## Results

Here are several example results using different models and prompts. Some of the prompts are taken from the paper results and will be denoted with an asterisk *.

### OpenAI (gpt-4o)

#### *Prompt 1: "Three colorful parrots perching on cherry blossom tree branch"

Raw output:

        -----Image Description-----

        Three colorful parrots perching on cherry blossom tree branch
        
        -----Model Output-----
        
        objects=[Object(name='Parrot 1', x0=70, y0=100, x1=170, y1=200), Object(name='Parrot 2', x0=200, y0=100, x1=300, y1=200), Object(name='Parrot 3', x0=330, y0=100, x1=430, y1=200), Object(name='Cherry Blossom Branch', x0=50, y0=250, x1=462, y1=290)]

![threeparrots](https://github.com/user-attachments/assets/09db991e-539c-48cb-830d-479e5a29f167)

#### Prompt 1: "two rabbits enjoying a birthday cake on a hill at sunset"

Raw output:

        -----Image Description-----

        two rabbits enjoying a birthday cake on a hill at sunset
        
        -----Model Output-----
        
        objects=[Object(name='Rabbit 1', x0=50, y0=256, x1=150, y1=356), Object(name='Rabbit 2', x0=190, y0=256, x1=290, y1=356), Object(name='Birthday Cake', x0=110, y0=306, x1=230, y1=356), Object(name='Sunset', x0=0, y0=0, x1=512, y1=150), Object(name='Hill', x0=0, y0=206, x1=512, y1=512)]

![rabbitcake](https://github.com/user-attachments/assets/10a4e3cb-31a5-4f26-8aa9-2bb0794a9327)



## Conclusions & Further Work

